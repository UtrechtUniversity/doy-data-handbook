[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dynamics of Youth",
    "section": "",
    "text": "Welcome!\nThe Data Handbook for Dynamics of Youth provides information and resources on research data management and making data pertaining to youth research Findable, Accessible, Interoperable, and Reusable (FAIR).\nThe Handbook need not be read from start to finish, like a textbook. You are invited to navigate to the topic you need based on the table of contents. As far as possible, the chapters have been written concisely - emphasizing practical tips or links to existing resources.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "definitions.html",
    "href": "definitions.html",
    "title": "Definitions",
    "section": "",
    "text": "Research Data Management\nBefore diving into data management, it would be good to get familiarized with some data-related terms that are oftentimes misunderstood or used interchangeably.\nResearch Data Management (RDM) refers to the active organization and maintenance of data created during a research project. It is an ongoing activity throughout the data lifecycle, from initial planning to suitable archiving of the data at the project’s completion.",
    "crumbs": [
      "Definitions"
    ]
  },
  {
    "objectID": "definitions.html#fair-data",
    "href": "definitions.html#fair-data",
    "title": "Definitions",
    "section": "FAIR Data",
    "text": "FAIR Data\nThe FAIR Data Principles are a set of guiding principles to improve scientific data management and stewardship (Wilkinson et al., 2016)\n\nFindability makes it possible for others to discover your data (metadata, Persistent Identifiers, etc.).\nAccessibility makes it possible for humans and machines to gain access to your data, under specific conditions or restrictions where appropriate.\nInteroperability ensures data and metadata conform to recognized formats and standards which allows them to be combined and exchanged.\nReusability requires lots of documentation, which is needed to support data and interpretation and reuse.",
    "crumbs": [
      "Definitions"
    ]
  },
  {
    "objectID": "definitions.html#open-data",
    "href": "definitions.html#open-data",
    "title": "Definitions",
    "section": "Open Data",
    "text": "Open Data\nOpen Data is data that can be freely used, re-used, and redistributed by anyone - subject only, at most, to the requirement to attribute and share-alike (Open Data Handbook).\nNote that your data does not have to be ‘open’ to be FAIR!\nMake your data… ‘as open as possible, as closed as necessary’ (European Commission).",
    "crumbs": [
      "Definitions"
    ]
  },
  {
    "objectID": "definitions.html#summary",
    "href": "definitions.html#summary",
    "title": "Definitions",
    "section": "Summary",
    "text": "Summary\nIn short,\n\nRDM = an activity/practice\nFAIR = principles that guide RDM activities/practices\nOpen Data = data does not have to be ‘open’ to be FAIR!",
    "crumbs": [
      "Definitions"
    ]
  },
  {
    "objectID": "data-management-plans.html",
    "href": "data-management-plans.html",
    "title": "Data Management Plans",
    "section": "",
    "text": "What\nA Data Management Plan (DMP) is a formal document that describes your data and outlines all aspects of managing your data - both during and after your project. It briefly describes what data you will collect, where and how it will be stored, accessed, backed up, documented and versioned, addresses any privacy or ownership issues, and explains your plans for archiving and sharing the data.",
    "crumbs": [
      "PLAN & FUND",
      "Data Management Plans"
    ]
  },
  {
    "objectID": "data-management-plans.html#why",
    "href": "data-management-plans.html#why",
    "title": "Data Management Plans",
    "section": "Why",
    "text": "Why\nWriting a DMP provides an opportunity to reflect on your data, particularly how you organize and manage it. It nudges you to think about how to make your RDM more concrete and actionable. This creates a roadmap for handling data during your project and identifies which resources are required. This results in more efficient project planning, saving both time and money. Additionally, most funders require researchers to submit a DMP.",
    "crumbs": [
      "PLAN & FUND",
      "Data Management Plans"
    ]
  },
  {
    "objectID": "data-management-plans.html#when",
    "href": "data-management-plans.html#when",
    "title": "Data Management Plans",
    "section": "When",
    "text": "When\nWorking on a DMP at the start of your project will ensure that you are better informed of best practices in RDM and prepared to implement them. That being said, you can also write a DMP during the project or when it’s completed. It is a living document that can you can revise and update as needed.",
    "crumbs": [
      "PLAN & FUND",
      "Data Management Plans"
    ]
  },
  {
    "objectID": "data-management-plans.html#how",
    "href": "data-management-plans.html#how",
    "title": "Data Management Plans",
    "section": "How",
    "text": "How\n\n\nDMPonline & DMP Templates\nDMPonline is a tool that helps you create and maintain DMPs. With DMPonline, you can:\n\nregister and sign in with your institutional credentials,\nwrite and collaborate on (multiple) DMPs,\nshare DMPs or switch their visibility between private and public,\nrequest feedback from RDM Support,\ndownload DMPs in various formats.\n\nDMPonline offers DMP templates from various institutions and funders, including:\n\nUtrecht University\nUMC Utrecht\nNWO\nZonMw\nERC\nHorizon 2020\nHorizon Europe\n\nThese templates also contain example answers and guidance.\n\n\n\n\n\n\nTip\n\n\n\n\nUse the Feedback button on DMPonline to request a review of your DMP from RDM Support.\nYou can export your DMP as a document and expand on it. This allows tailoring the DMP to suit your project, beyond the given template.",
    "crumbs": [
      "PLAN & FUND",
      "Data Management Plans"
    ]
  },
  {
    "objectID": "data-management-plans.html#resources",
    "href": "data-management-plans.html#resources",
    "title": "Data Management Plans",
    "section": "Resources",
    "text": "Resources\n\nLearn to write your DMP (online training)\nCreate your DMP online\nData management planning",
    "crumbs": [
      "PLAN & FUND",
      "Data Management Plans"
    ]
  },
  {
    "objectID": "data-management-plans.html#references",
    "href": "data-management-plans.html#references",
    "title": "Data Management Plans",
    "section": "References",
    "text": "References\n\nhttps://www.uu.nl/en/research/research-data-management/guides/data-management-planning\nhttps://www.kuleuven.be/rdm/en/faq/faq-dmp\nhttps://rdm.uva.nl/en/planning/data-management-plan/data-management-plan.html\nhttps://www.uu.nl/en/research/research-data-management/tools-services/tool-to-create-your-dmp-online.html",
    "crumbs": [
      "PLAN & FUND",
      "Data Management Plans"
    ]
  },
  {
    "objectID": "privacy-and-security.html",
    "href": "privacy-and-security.html",
    "title": "Privacy & Security",
    "section": "",
    "text": "What\nPrivacy focuses on minimising and protecting identifiable information. It includes:\nSecurity focuses on safeguarding data against loss, misuse, or unauthorised access. It includes:",
    "crumbs": [
      "PLAN & FUND",
      "Privacy & Security"
    ]
  },
  {
    "objectID": "privacy-and-security.html#what",
    "href": "privacy-and-security.html#what",
    "title": "Privacy & Security",
    "section": "",
    "text": "collecting only the personal data you truly need\nkeeping identifiers separate from research data\nusing pseudonymisation as early as possible\nensuring appropriate consent/assent and necessary agreements are in place\nlimiting access to identifiable data to authorised team members\ndocumenting decisions about what data you collect and why\ndeleting or anonymising data when it is no longer needed\n\n\n\nstoring data in secure, approved environments\nusing secure methods for sharing and transferring data, such as encryption\navoiding personal devices and unsupported cloud tools\nrestricting access to authorised users only\nkeeping secure logs of decisions and data-handling procedures\nensuring data is handled and transferred through trusted channels",
    "crumbs": [
      "PLAN & FUND",
      "Privacy & Security"
    ]
  },
  {
    "objectID": "privacy-and-security.html#why",
    "href": "privacy-and-security.html#why",
    "title": "Privacy & Security",
    "section": "Why",
    "text": "Why\nFollowing best practices in privacy and security ensures that personal data - especially involving children and adolescents - are handled safely, lawfully, and ethically. These practices are relevant to every stage of your project: from collection to storage, processing, and sharing. Paying attention to these practices safeguards participants’ rights, reduces the risk of data breaches, and protects the integrity of your research. It ensures compliance with the GDPR (General Data Protection Regulation), as well as relevant institutional policies.",
    "crumbs": [
      "PLAN & FUND",
      "Privacy & Security"
    ]
  },
  {
    "objectID": "privacy-and-security.html#when",
    "href": "privacy-and-security.html#when",
    "title": "Privacy & Security",
    "section": "When",
    "text": "When\nBeing mindful of privacy and security from the start will improve your workflow and reduce risks later on. That’s why you should address these issues during the planning phase of your project, before even collecting any data . This is the moment to determine what personal data you will collect, whether a Data Protection Impact Assessment (DPIA) is required, and which secure systems you will use.\nStill, privacy and security are not a one-time check. You may need to revisit them during the project, particularly when new partners join, data flows shift, or additional types of data are introduced.",
    "crumbs": [
      "PLAN & FUND",
      "Privacy & Security"
    ]
  },
  {
    "objectID": "privacy-and-security.html#how",
    "href": "privacy-and-security.html#how",
    "title": "Privacy & Security",
    "section": "How",
    "text": "How\nThe one-pager below outlines the basic steps in every research project that uses personal data, please refer to the Data Privacy Handbook to find more information about each of these topics.\n\n\n\nUtrecht University RDM Support (2023). 10 steps towards privacy compliance in research. https://doi.org/10.5281/zenodo.10417513",
    "crumbs": [
      "PLAN & FUND",
      "Privacy & Security"
    ]
  },
  {
    "objectID": "privacy-and-security.html#resources",
    "href": "privacy-and-security.html#resources",
    "title": "Privacy & Security",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\nNote\n\n\n\nThe information below is borrowed from the Seeking help at Utrecht University chapter from the Data Privacy Handbook.\n\n\nIn addition to the Data Privacy Handbook, there are several resources available to support you with building and maintaining privacy & security into your research projects:\n\nEducation\nPrivacy basics for researchers (online training)\n\n\nTools & Services\n\nThe UU Tooladvisor lists tools that are GDPR-compliant and safe to use.\nThe webpage on Privacy Engineering Tools outlines tools and packages to deidentify, encrypt, synthetise and otherwise work with personal data.\n\n\n\nOnline Information\nUU-wide\n\nWebsite: Research Data Management Support\nIntranet: Privacy (UU?)\nIntranet: Information Security (UU?)\n\nFaculty-Specific\n\nGeosciences: RDM and privacy, ethics\nScience: RDM and privacy, ethics\nSocial and Behavioural Sciences: tech support, ethics\nHumanities: RDM and privacy, ethics\nLaw, Economics and Governance: RDM and privacy, ethics\nVeterinary medicine: Research Support Office\nMedicine: ethics, data management-related information can be found on UMCU Connect.\n\n\n\nIn-Person Support\nThe first point of contact about privacy are the Privacy Officers of your faculty.\nBesides the privacy officer, you can also ask for help from:\n\nYour local data steward/data manager (see websites above) or Research Data Management Support.\nInformation security.\nIn some faculties, the Research Support Officemay be of help in drafting agreements.\nIf you suspect a data leak or data breach, contact the Service Deskimmediately.",
    "crumbs": [
      "PLAN & FUND",
      "Privacy & Security"
    ]
  },
  {
    "objectID": "costs.html",
    "href": "costs.html",
    "title": "Costs",
    "section": "",
    "text": "What\nData management costs include all expenses related to planning, collecting, storing, analysing, publishing, sharing and archiving data for a project. This encompasses licenses for data collection or reuse, tools and software, and any expertise needed to support the research.",
    "crumbs": [
      "PLAN & FUND",
      "Costs"
    ]
  },
  {
    "objectID": "costs.html#why",
    "href": "costs.html#why",
    "title": "Costs",
    "section": "Why",
    "text": "Why\nExplicitly considering data management expenses ensures that necessary resources are available throughout the project. Many date-related activities require time, infrastructure, and expertise. This can lead to unanticipated expenses if not planned in advance. Transparent budgeting helps avoid delays, supports ensures compliance with funder and institutional requirements, and supports sustainable data practices.",
    "crumbs": [
      "PLAN & FUND",
      "Costs"
    ]
  },
  {
    "objectID": "costs.html#when",
    "href": "costs.html#when",
    "title": "Costs",
    "section": "When",
    "text": "When\nPlan data management costs early - ideally during grant preparation - using a preliminary DMP to make storage, documentation, analysis, backup, and archiving needs explicit. This can serve as a basis for the Costs section in the offical DMP.",
    "crumbs": [
      "PLAN & FUND",
      "Costs"
    ]
  },
  {
    "objectID": "costs.html#who",
    "href": "costs.html#who",
    "title": "Costs",
    "section": "Who",
    "text": "Who\nThe research team is primarily responsible for identifying and budgeting data management costs - as they best understand the scope and requirements of their project. Institutional support is available from data stewardship teams, libraries, and IT services - who advise on expected expenses and available infrastructure. Funders typically accept data management costs as part of grants, so they should be included in the proposal and planned accordingly.",
    "crumbs": [
      "PLAN & FUND",
      "Costs"
    ]
  },
  {
    "objectID": "costs.html#where",
    "href": "costs.html#where",
    "title": "Costs",
    "section": "Where",
    "text": "Where\nThe Data Management Plan (DMP) is a suitable place to outline data management expenses. Most templates include dedicated sections for costs and resources, making it straightforward to document your planning.",
    "crumbs": [
      "PLAN & FUND",
      "Costs"
    ]
  },
  {
    "objectID": "costs.html#how",
    "href": "costs.html#how",
    "title": "Costs",
    "section": "How",
    "text": "How\n\nIdentify the data-related tasks required at each stage of the project (e.g., collecting, transcribing, anonymizing, structuring, documenting, sharing, archiving) and estimate the time and resources required. You can use the recommended hourly rates for student assistants or data professionals where possible.\nWhenever feasible, use institutional or open-source tools and infrastructure instead of expensive external services, and rely on templates, standards, and standard operating procedures to increase efficiency and reduce costs.",
    "crumbs": [
      "PLAN & FUND",
      "Costs"
    ]
  },
  {
    "objectID": "costs.html#references",
    "href": "costs.html#references",
    "title": "Costs",
    "section": "References",
    "text": "References\n\nhttps://www.uu.nl/en/research/research-data-management/guides/costs-of-data-management\nhttps://www.utwente.nl/en/dcc/news/2022/10/135327/new-guide-how-to-estimate-research-data-management-costs-for-your-research-proposal\nhttps://www.tudelft.nl/library/data-management/onderzoeksdata-beheren/de-kosten-van-data-management",
    "crumbs": [
      "PLAN & FUND",
      "Costs"
    ]
  },
  {
    "objectID": "data-flow-diagrams.html",
    "href": "data-flow-diagrams.html",
    "title": "Data Flow Diagrams",
    "section": "",
    "text": "What\nA data flow diagram (DFP) is a visual representation of the flow of data through a process or system. It provides an overview of incoming and outgoing data, as well as the processing and tools involved.",
    "crumbs": [
      "PLAN & FUND",
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "data-flow-diagrams.html#why",
    "href": "data-flow-diagrams.html#why",
    "title": "Data Flow Diagrams",
    "section": "Why",
    "text": "Why\nA DFD is valuable because it provides an outline of your data processes. It allows you to see how these processes interact and identify opportunities for improvement. For example, it can provide the basis for developing your data pipeline.",
    "crumbs": [
      "PLAN & FUND",
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "data-flow-diagrams.html#when",
    "href": "data-flow-diagrams.html#when",
    "title": "Data Flow Diagrams",
    "section": "When",
    "text": "When\nIt is recommended that you sketch a DPD while working on your DMP. Like the DMP, the DFD can be considered a living document. In DMPonline, you can export your DMP to docx and you can expand the document to include the DFD.",
    "crumbs": [
      "PLAN & FUND",
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "data-flow-diagrams.html#how",
    "href": "data-flow-diagrams.html#how",
    "title": "Data Flow Diagrams",
    "section": "How",
    "text": "How\nDFDs can be as simple as hand-drawn flowcharts on an A4 sheet of paper to elaborate flowcharts with different symbols and markers.\n\nTools\nThe following tools are suitable to create DFDs:\n\nMicrosoft Visio (available via the Self Service Portal, choose Applications -&gt; Workplace applications -&gt; Request/Aanvraag)\nMicrosoft PowerPoint\nMermaid, a diagramming & charting tool that can be used with:\n\nQuarto\nDiagrammeR in R\nmermaid-py in Python\n\n\n\n\nExamples\n\nHear, Hear! #1\n\nResearchers: Inge van der Valk (FSW), Rianne van Dijk (FSW), Charlotte Mol (REBO), Zoë Rejaän (FSW)\nPrivacy Officers: Joris de Graaf (REBO) & Jacqueline Tenkink (FSW) \n\n\n\n\nData Processing for Children & Parents in the Hear, Hear! Project\n\n\n\n\nHear, Hear! #2\n\nResearchers: Inge van der Valk (FSW), Rianne van Dijk (FSW), Charlotte Mol (REBO), Zoë Rejaän (FSW)\nPrivacy Officers: Joris de Graaf (REBO) & Jacqueline Tenkink (FSW) \n\n\n\n\nData Processing for Professionals in the Hear, Hear! Project\n\n\n\n\nYOUth Cohort Study\nZondergeld, J. J., Scholten, R. H. H., Vreede, B. M. I., Hessels, R. S., Pijl, A. G., Buizer-Voskamp, J. E., Rasch, M., Lange, O. A., & Veldkamp, C. L. S. (2020). FAIR, safe and high-quality data: The data infrastructure and accessibility of the YOUth cohort study. Developmental Cognitive Neuroscience, 45, 100834. https://doi.org/10.1016/j.dcn.2020.100834\n\n\n\nOverview of the infrastructure used in the YOUth Cohort Study, illustrating the systems involved and the flow of data between them\n\n\n\n\nSmart-Youth\n\nResearchers: Isabelle van der Linden (WKZ), Henk Schipper (WKZ), Sanne Nijhof (WKZ), Kors van der Ent (WKZ)\nData Manager: Neha Moopen (UBU)\n\n\n\n\nSmart-Youth DFD\n\n\n\n\nPrenatal Family Integrated Care (PFIC)\n\nResearcher: Neeltje Crombag\nData Manager: Neha Moopen (UBU)\n\n\n\n\nPFIC DFD",
    "crumbs": [
      "PLAN & FUND",
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "data-flow-diagrams.html#resources",
    "href": "data-flow-diagrams.html#resources",
    "title": "Data Flow Diagrams",
    "section": "Resources",
    "text": "Resources\n\nCreating a data-flow diagram from Elixir Europe’s RDM Kit.",
    "crumbs": [
      "PLAN & FUND",
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "naming-conventions.html",
    "href": "naming-conventions.html",
    "title": "Naming Conventions",
    "section": "",
    "text": "What Is A Naming Convention?\nA naming convention is a set of rules for naming things. You can apply it to things like folders, files, and variables.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#why-should-i-apply-a-naming-convention",
    "href": "naming-conventions.html#why-should-i-apply-a-naming-convention",
    "title": "Naming Conventions",
    "section": "Why Should I Apply A Naming Convention?",
    "text": "Why Should I Apply A Naming Convention?\nNames that are informative and useful for machines and humans are a step toward efficient data management and reproducible research. The more consistent and meaningful the name, the easier it will be to locate and identify things, understand what they contain, and (re)use them.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#when-should-i-apply-a-naming-convention",
    "href": "naming-conventions.html#when-should-i-apply-a-naming-convention",
    "title": "Naming Conventions",
    "section": "When Should I Apply A Naming Convention?",
    "text": "When Should I Apply A Naming Convention?\nAim to select and implement a naming convention at the beginning of a project. If you want to retroactively apply a naming convention, there are several tools for bulk renaming.\nThe entire research team should agree on and adopt a naming convention. Document the choice of naming convention in the DMP, so others can refer to and grasp it quickly.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#popular-naming-conventions",
    "href": "naming-conventions.html#popular-naming-conventions",
    "title": "Naming Conventions",
    "section": "Popular Naming Conventions",
    "text": "Popular Naming Conventions\nInstead of developing a naming convention from scratch, you can start with one that is already being used in programming and software development communities:\n\n\n\n\n\n\n\n\nNaming Covention\nExample\nDescription\n\n\n\n\noriginal name\nan awesome name\nN/A\n\n\nsnake_case\nan_awesome_name\nAll words are lowercase and separated by an underscore ( _ )\n\n\nkebab-case\nan-awesome-name\nAll words are lowercase and separated by a hyphen ( - )\n\n\nPascalCase\nAnAwesomeName\nAll words are capitalized. Spaces are not used.\n\n\ncamelCase\nanAwesomeName\nThe first word is lowercase, the remaining words are capitalized. Spaces are not used.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#human-readable-names",
    "href": "naming-conventions.html#human-readable-names",
    "title": "Naming Conventions",
    "section": "Human-Readable Names",
    "text": "Human-Readable Names\nYou can tailor naming conventions like snake_case and PascalCase to suit your project and workflow. Determine what information is relevant (or not) to create meaningful names and how you can string this information together. Don’t forget to document this in your DMP!\n!!! note “Elements for Human-Readable Names”\nNames should be =&lt;25 characters long and can include:\n\n- Date of creation/update (`YYYY-MM-DD` or `YYYYMMDD`)\n- Description of content, like type of data\n- Initials of creator/reviewer\n- Project number or acronym\n- Location/coordinates\n- Version number (like `v2` or v2.2`)",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#machine-readable-names",
    "href": "naming-conventions.html#machine-readable-names",
    "title": "Naming Conventions",
    "section": "Machine-Readable Names",
    "text": "Machine-Readable Names\nWhen names are machine-readable, they can be efficiently processed by computers and software. This makes it easier to search for files and run operations that involve programming like extracting information from file names or working with regular expressions.\n!!! note “Avoid”\n- Spaces\n- Special characters like `$`, `@`, `%`, `#`, `&`, `*`, `!`, `/`, `\\`\n- Punction characters like `,`, `:`, `;`, `?`, `'`, `\"`\n- Accented characters",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#a-note-on-numbering-dates-versioning",
    "href": "naming-conventions.html#a-note-on-numbering-dates-versioning",
    "title": "Naming Conventions",
    "section": "A Note on Numbering, Dates, Versioning",
    "text": "A Note on Numbering, Dates, Versioning\n\nAppend numbers to the beginning of a name to enable sorting according to a logical structure. Use multiple digits like 01 or 001.\nDates should follow the ISO 8601 standard which is either YYYY-MM-DD or YYYYMMDD. Append dates to the beginning of names to enable sorting in chronological order.\nSpecify versions using ordinal numbers (1,2,3) for major revisions and decimals for minor changes (1.1, 1.2, 2.1, 2.2). Alternatively, you can specify versions with multiple digits like v01 and v02.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#renaming-files",
    "href": "naming-conventions.html#renaming-files",
    "title": "Naming Conventions",
    "section": "Renaming files",
    "text": "Renaming files\nThe following tools enable renaming in bulk:\n\nBulk Rename Utility (Windows, free)\nRenamer (MacOS, paid)\nNameChanger, (MacOS, free)\nGPRename (Linux, free)",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "naming-conventions.html#references",
    "href": "naming-conventions.html#references",
    "title": "Naming Conventions",
    "section": "References",
    "text": "References\n\nhttps://en.wikipedia.org/wiki/Naming_convention\nhttps://help.osf.io/article/146-file-naming\nhttps://rdm.elixir-belgium.org/file_naming.html\nhttps://khalilstemmler.com/blogs/camel-case-snake-case-pascal-case/\nhttps://dev.to/chaseadamsio/most-common-programming-case-types-30h9\nhttps://rdmkit.elixir-europe.org/data_organisation http://dataabinitio.com/?p=987\nhttps://dmeg.cessda.eu/Data-Management-Expert-Guide/2.-Organise-Document/File-naming-and-folder-structure\nhttps://annakrystalli.me/rrresearchACCE20/filenaming-view.html",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "folder-structures.html",
    "href": "folder-structures.html",
    "title": "Folder Structures",
    "section": "",
    "text": "How\nWhether it’s on your computer or in the cloud, your project will likely be organized into folders and subfolders.\nA clear structure keeps all your materials grouped in a predictable way. It helps you know exactly where each type of file belongs, making it easier to organize and locate them. It also keeps things aligned when multiple people contribute to the same project.\nThe project lead should define the folder structure — this could be the PI for a larger project or an individual researcher for a manuscript. You should also ensure that collaborators are informed about the structure and follow it accordingly.\nThis should be done at the beginning of a project, but there is always room to update and/or reorganize things along the way.\nin the storage location recommended for your project or in your archive/repository\ncategorize things as much as possible and have a main folder and perhaps 3 sub-folders. it’s not struct and can be customaized, the aim is to keep it simple and don’t overcomplicate..don’t forget to use the naming convention you decided on earlier.\nwe can distinguish project-level vs. publication-level.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Folder Structures"
    ]
  },
  {
    "objectID": "folder-structures.html#how",
    "href": "folder-structures.html#how",
    "title": "Folder Structures",
    "section": "",
    "text": "project\nUMCU has the RFS..there is the Gen R fodler strcture, TIER protocol, BIDS that you can take inpiration from…based on that here is a recommendation\n\n\npublication\nfollow the best practices from computational reproducibility / cookiecutter, primarily for programming projects but can be adapted for all analysis projects",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Folder Structures"
    ]
  },
  {
    "objectID": "folder-structures.html#tools",
    "href": "folder-structures.html#tools",
    "title": "Folder Structures",
    "section": "Tools",
    "text": "Tools\nhere is are .zip files with the aforementioned folder streucture\nyou can also generate the folder structure using this command\nyou can also use the tree command to print the directory, you can use it for inspection or copy-paste into readme files",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Folder Structures"
    ]
  },
  {
    "objectID": "folder-structures.html#resources",
    "href": "folder-structures.html#resources",
    "title": "Folder Structures",
    "section": "Resources",
    "text": "Resources\nlinks to protocols? UMCU has the RFS..there is the Gen R fodler strcture, TIER protocol, BIDS that you can take inpiration from…\nhere are some analysis organization inspo",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Folder Structures"
    ]
  },
  {
    "objectID": "folder-structures.html#references",
    "href": "folder-structures.html#references",
    "title": "Folder Structures",
    "section": "References",
    "text": "References\ndirect references…",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Folder Structures"
    ]
  },
  {
    "objectID": "preferred-formats.html",
    "href": "preferred-formats.html",
    "title": "Preferred Formats",
    "section": "",
    "text": "source: FOSTER Open Science Training Handbook.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Preferred Formats"
    ]
  },
  {
    "objectID": "pipelining.html",
    "href": "pipelining.html",
    "title": "Data Pipelining",
    "section": "",
    "text": "When do I need a data pipeline?\nA data pipeline is a series of (automated) actions that ingests raw data from various sources and moves the data to a destination for storage and (eventual) analysis.\nBenefits of a data pipeline include:\nHere’s a rule of thumb, just as an example:\nIf you have a task that needs to occur &gt;= 3 times, you could think about automating it.\nIf automation is not possible, think about how you can make the task as efficient as possible.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Data Pipelining"
    ]
  },
  {
    "objectID": "pipelining.html#how-can-i-implement-a-data-pipeline-some-examples-for-inspiration",
    "href": "pipelining.html#how-can-i-implement-a-data-pipeline-some-examples-for-inspiration",
    "title": "Data Pipelining",
    "section": "How can I implement a data pipeline? Some examples for inspiration",
    "text": "How can I implement a data pipeline? Some examples for inspiration\n\nIf you data collection tools have APIs, they can be leveraged to extract data.\nFor example, Qualtrics has the qualtRics R package & pyQualtrics Python library which contain functions to automate exporting surveys.\nIf APIs are not available, you could use R/Python to automate the use of an internet browser using the RSelenium package / Selenium library. Imagine automating the clicks and typing of going to a specific website, logging in, clicking the download button.\nYou can use Windows Task Scheduler / cron / the taskscheduleR R package / cronR to schedule your scripts to run automatically, on a recurring basis as well (if needed).\nYou can also send emails with R & Python! Consider if you’ve ever had to contact participants because you noticed something wrong with their incoming data. You could implement these data checks with a script and automatically draft and send emails (from a template) to those participants who were flagged as having issues with their data.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Data Pipelining"
    ]
  },
  {
    "objectID": "pipelining.html#qualtrics-r-package",
    "href": "pipelining.html#qualtrics-r-package",
    "title": "Data Pipelining",
    "section": "QualtRics R package",
    "text": "QualtRics R package\nlibrary(readr)\nlibrary(qualtRics)\n\nqualtrics_api_credentials(api_key = \"YOUR-QUALTRICS-API-KEY\", \n                          base_url = \"YOUR-QUALTRICS-BASE-URL\",\n                          overwrite = TRUE,\n                          install = TRUE)\n\nreadRenviron(\"~/.Renviron\")\n\nsurveys &lt;- all_surveys() \n\nsurvey_results &lt;- fetch_survey(surveyID = surveys$id[2], # you can also replace surveys$id[2] with \"&lt;SUVREY-ID&gt;\" \n                                  verbose = TRUE)\n\nwrite_csv(survey_results, paste0(\"path/to/folder/\", format(Sys.time(), \"%d-%m-%Y-%H.%M\"), \"_survey_results.csv\"))",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Data Pipelining"
    ]
  },
  {
    "objectID": "pipelining.html#taskscheduler-package",
    "href": "pipelining.html#taskscheduler-package",
    "title": "Data Pipelining",
    "section": "taskscheduleR package",
    "text": "taskscheduleR package\nlibrary(taskscheduleR)\n\nscheduled_script &lt;- \"path/to/folder/myscript.R\"\n\n## run script once within 120 seconds\n\ntaskscheduler_create(taskname = \"extract-data-once\", rscript = scheduled_script,\n                     schedule = \"ONCE\", starttime = format(Sys.time() + 120, \"%H:%M\"))\n\n## Run every 5 minutes, starting from 10:40\n\ntaskscheduler_create(taskname = \"extract-data-5min\", rscript = scheduled_script,\n                     schedule = \"MINUTE\", starttime = \"10:40\", modifier = 5)\n\n## delete tasks\n\ntaskscheduler_delete(\"extract-data-once\")",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Data Pipelining"
    ]
  },
  {
    "objectID": "metadata.html",
    "href": "metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "Project-Level Metadata\nMetadata is structured information that describes one or more aspects of your research data. In other words, metadata = ‘data about data’. Metadata is machine-readable and helps make your data findable and citable.\nMetadata exists at different levels:\nThis type of metadata describes higher-order aspects of your dataset: the “who, what, where, when, how and why” … It provides context for understanding why the data were collected and how they were used.\n• Name of the project • Dataset title • Project description • Dataset abstract • Principal investigator and collaborators • Contact information • Dataset handle (DOI or URL) • Dataset citation • Data publication date • Geographic description • Time period of data collection • Subject/keywords • Project sponsor • Dataset usage rights",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Metadata"
    ]
  },
  {
    "objectID": "metadata.html#data-level-metadata",
    "href": "metadata.html#data-level-metadata",
    "title": "Metadata",
    "section": "Data-Level Metadata",
    "text": "Data-Level Metadata\n• Data origin: experimental, observational, raw or derived, physical collections, models, images, etc. • Data type: integer, Boolean, character, floating point, etc. • Instrument(s) used • Data acquisition details: sensor deployment methods, experimental design, sensor calibration methods, etc. • File type: CSV, mat, xlsx, tiff, HDF, NetCDF, etc. • Data processing methods, software used • Data processing scripts or codes • Dataset parameter list, including ⚬ Variable names ⚬ Description of each variable ⚬ Units\nThis type of metadata is more granular and describes the data (variables) and dataset in detail.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Metadata"
    ]
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation Checklist\nDocumentation refers to contextual information pertaining to your research data. It accompanies (structured) metadata and guides users to understand and interpret your data and reuse it effectively.\nDocumentation is meant to be human-readable and it is a crucial aspect of interoperability and reusability. Some examples include:\n• Grant / Study Proposals • Study Protocol / Methodology • Data Management Plan (DMP) • README files • Lab Notebooks • Legal / Policy / Administrative Documents\nHere is a starter checklist to make an inventory of your documentation: https://tinyurl.com/documentation-checklist",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Documentation"
    ]
  },
  {
    "objectID": "codebooks.html",
    "href": "codebooks.html",
    "title": "Codebooks",
    "section": "",
    "text": "codebook R package\nA codebook is an example of data-level metadata.\nThe purpose of a codebook or data dictionary is to explain what all the variable names and values in your spreadsheet really mean.\nInformation to include in a codebook includes:\nSee: https://help.osf.io/article/217-how-to-make-a-data-dictionary\nThe labelled R package can also do something similar.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Codebooks"
    ]
  },
  {
    "objectID": "codebooks.html#codebook-r-package",
    "href": "codebooks.html#codebook-r-package",
    "title": "Codebooks",
    "section": "",
    "text": "library(qualtRics)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(codebook)\nlibrary(writexl)\n\nsurveys &lt;- all_surveys()\n\nsurvey_results &lt;- fetch_survey(surveyID = surveys$id[2], # you can also replace surveys$id[2] with \"&lt;SUVREY-ID&gt;\"\n                               verbose = TRUE)\n\nsurvey_results &lt;- select(survey_results, -c(1:17))\n\n# survey_questions() retrieves a data frame containing questions and question IDs for a survey;\nsurvey_questions &lt;- survey_questions(surveyID = surveys$id[2])\nsurvey_questions &lt;- select(survey_questions, -c(1, 4))\nsurvey_questions &lt;- slice(survey_questions, -1)\n  \n# generate codebook\n\ncodebook &lt;- codebook_table(survey_results)\n\ncodebook &lt;- rename(codebook, qname = name)\n\ncodebook &lt;- full_join(survey_questions, codebook, by = \"qname\")\n\nwrite_xlsx(codebook, \"documentation/codebook-demo.xlsx\")",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Codebooks"
    ]
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "Reproducibility",
    "section": "",
    "text": "The Turing Way project illustration by Scriberia. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807.",
    "crumbs": [
      "COLLECT & ANALYZE",
      "Reproducibility"
    ]
  },
  {
    "objectID": "storage.html",
    "href": "storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "What\nWhen discussing storage, we refer to the location of ‘active’ data, which is in use and may change throughout the research project. The related concepts of archiving and publishing refer to where the data will be saved or deposited after the project is completed.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Storage"
    ]
  },
  {
    "objectID": "storage.html#why",
    "href": "storage.html#why",
    "title": "Data Storage",
    "section": "Why",
    "text": "Why\nStoring data properly ensures that research can be conducted efficiently, prevents data loss, and maintains the integrity and reproducibility of results. Well-managed storage supports collaboration, version control, and easy retrieval of files during the project.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Storage"
    ]
  },
  {
    "objectID": "storage.html#who",
    "href": "storage.html#who",
    "title": "Data Storage",
    "section": "Who",
    "text": "Who\nWhen planning data storage, consider collaboration with other researchers. Decisions about who will access the data and how often it will be shared affect both where the data is stored and how it is organized, backed up, and structured.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Storage"
    ]
  },
  {
    "objectID": "storage.html#when",
    "href": "storage.html#when",
    "title": "Data Storage",
    "section": "When",
    "text": "When\nA decision about data storage needs to be taken at the onset of the project. From that point, data should be stored securely and include regular backups to prevent loss or accidental overwriting.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Storage"
    ]
  },
  {
    "objectID": "storage.html#where",
    "href": "storage.html#where",
    "title": "Data Storage",
    "section": "Where",
    "text": "Where\nThe Data Storage Finder is a tool provided by IT help you decide which storage solution would be most suited to your needs. The suggestions are based on factors such as collaboration, size, sensitivity, and (ease of) access.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Storage"
    ]
  },
  {
    "objectID": "storage.html#how",
    "href": "storage.html#how",
    "title": "Data Storage",
    "section": "How",
    "text": "How\nWhen storing data, consider the following:\n\nchoose storage media that is appropriate for the type of data you’re working with\nensure reliable version control and backups\nfollow a naming convention\nstructure folders and organize files clearly\nuse preferred and sustainable file formats\nsecure data files, using access control and/or encryption",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Storage"
    ]
  },
  {
    "objectID": "archiving.html",
    "href": "archiving.html",
    "title": "Data Archiving",
    "section": "",
    "text": "What\nData archiving is the long-term, secure preservation of research data. At this stage, the data is no longer being ‘actively’ used.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Archiving"
    ]
  },
  {
    "objectID": "archiving.html#why",
    "href": "archiving.html#why",
    "title": "Data Archiving",
    "section": "Why",
    "text": "Why\nArchiving enables future verification and safeguards the integrity of research. By storing raw data in a stable and accessible manner, researchers and institutions can revisit it if questions arise about methods, results, or conclusions.\nArchiving is not directly part of the FAIR principles, which focus on sharing and reusing data. Nonetheless, good archiving practices - such as organized file structures and clear documentation - provide a strong foundation for later FAIRification.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Archiving"
    ]
  },
  {
    "objectID": "archiving.html#who",
    "href": "archiving.html#who",
    "title": "Data Archiving",
    "section": "Who",
    "text": "Who\nThe lead researcher is primarily responsibility for ensuring data is archived, while the research team shares responsibility for preparing and organizing the files.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Archiving"
    ]
  },
  {
    "objectID": "archiving.html#when",
    "href": "archiving.html#when",
    "title": "Data Archiving",
    "section": "When",
    "text": "When\nArchive your data at the end of your project, when no further changes are expected to be made. This can be when data collection and analysis are complete, or when results are published or submitted.\nRetention requirements vary by institution, funder, and discipline. A common guideline is to preserve raw data for at least 10 years after project completion or publication, though some fields require longer retention for legal, ethical, or scientific reasons.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Archiving"
    ]
  },
  {
    "objectID": "archiving.html#where",
    "href": "archiving.html#where",
    "title": "Data Archiving",
    "section": "Where",
    "text": "Where\nArchive your data in the Vault area of YODA to create a read-only, long-term snapshot that cannot be overwritten or deleted, ensuring data integrity.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Archiving"
    ]
  },
  {
    "objectID": "archiving.html#how",
    "href": "archiving.html#how",
    "title": "Data Archiving",
    "section": "How",
    "text": "How\nSelect the data to preserve long-term, as archiving all research materials and data (especially large datasets not directly related to the project) can be practically and financially challenging.\nTo improve future reusability, store data in recommended formats that are non-proprietary, unencrypted, and uncompressed.",
    "crumbs": [
      "PRESERVE & STORE",
      "Data Archiving"
    ]
  },
  {
    "objectID": "governance.html",
    "href": "governance.html",
    "title": "Data Governance",
    "section": "",
    "text": "What\nData Governance defines how data are managed beyond the scope of the original project by setting out roles and responsibilities, as well as processes and policies. It ensures data are handled consistently and responsibly in support of ethical use, compliance, and long-term value.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Governance"
    ]
  },
  {
    "objectID": "governance.html#why",
    "href": "governance.html#why",
    "title": "Data Governance",
    "section": "Why",
    "text": "Why\nWithout clear guidance, data sharing and reuse can become time-consuming and easily deprioritized. A data governance framework reduces ad hoc communication and provides a standard operating procedure for making decisions and responding to requests, reducing friction and making follow-through more manageable.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Governance"
    ]
  },
  {
    "objectID": "governance.html#who",
    "href": "governance.html#who",
    "title": "Data Governance",
    "section": "Who",
    "text": "Who\nAll members of the research team involved in handling data should contribute to or be consulted when defining the framework. The principal investigator (PI) plays a central role in establishing and upholding data governance.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Governance"
    ]
  },
  {
    "objectID": "governance.html#when",
    "href": "governance.html#when",
    "title": "Data Governance",
    "section": "When",
    "text": "When\nIt is most effective to develop the framework in detail as data sharing and reuse approach. However, discussing goals and aspirations earlier in the project helps ensure that research data management decisions support these long-term objectives.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Governance"
    ]
  },
  {
    "objectID": "governance.html#where",
    "href": "governance.html#where",
    "title": "Data Governance",
    "section": "Where",
    "text": "Where\nDocuments related to data governance can be made publicly available alongside the data package in the chosen repository, providing transparency and guidance for future users.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Governance"
    ]
  },
  {
    "objectID": "governance.html#how",
    "href": "governance.html#how",
    "title": "Data Governance",
    "section": "How",
    "text": "How\nWhen you’re ready to start sharing your data, you can set up a detailed Data Access Protocol (DAP) that outlines data governance for yourself, your research team, and potential re-users.\nA DAP can cover many topics, and it will require you and/or your project team to decide what is relevant and appropriate for your data. You can make it as simple or as elaborate as you like.\nIt would help to reflect on the following points:\n\nGoals: What would you like to achieve by sharing your data? Some examples include citations/acknowledgments, co-authorship, or collaboration. Specify this in the DAP so end-users understand their obligations.\nResources: How much time and effort can you and/or your team invest in data governance? Consider tasks such as assessing incoming requests, preparing datasets for sharing, and maintaining a data sharing logbook. Note: If privacy-sensitive data are involved, even the simplest DAPs must consider legal requirements.\n\nSome suggestions for sections in your DAP include:\n\nData Ownership / License & Copyright\nRoles & Responsibilities\nTerms & Conditions\nData Request & Review Procedure\nPublication & Authorship Guidelines\nDisclaimers & Liabilities\n\nYou may also want to provide documents such as a Data Request Form and Publication Checklist for end users. These can be included in the DAP appendices and made available separately for easy access.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Governance"
    ]
  },
  {
    "objectID": "governance.html#examples",
    "href": "governance.html#examples",
    "title": "Data Governance",
    "section": "Examples",
    "text": "Examples\n\nUU/UMCU Projects:\n\nPROactive Cohort Study’s Data Access Protocol\nYOUth Cohort Study’s Data Access & Publication Guidelines\n\n\n\nNon-UU/UMCU Projects:\n\nL-CID’s Data Sharing Protocol, scroll down to Request Access To Our Data\nNTR’s Data Sharing Procedures\nTRAILS’ Data Request & Publication Plan",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Governance"
    ]
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Data Publication",
    "section": "",
    "text": "What\nWith Data Publishing, we’re making (meta)data findable and reusable for the public beyond the original project. This is different from Data Archiving, which is primarily internal and aimed at verifiability and integrity.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "publication.html#what",
    "href": "publication.html#what",
    "title": "Data Publication",
    "section": "",
    "text": "Tip\n\n\n\nRemember that your data does not have to be ‘open’ to be FAIR!\nYou want to make your data… ‘as open as possible, as closed as necessary’ (European Commission).",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "publication.html#why",
    "href": "publication.html#why",
    "title": "Data Publication",
    "section": "Why",
    "text": "Why\nThe (meta)data can be reused by others for their own research purposes, supporting collaboration and open science. Increasingly, funders and journals also require published (meta)data or data availability statements as part of research outputs.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "publication.html#who",
    "href": "publication.html#who",
    "title": "Data Publication",
    "section": "Who",
    "text": "Who\nTypically, data publishing is overseen by Principal Investigator (PI) for a large project or the lead researcher for a manuscript. You can approach a data supporter at your faculty or the University Library to help you with preparing a data package.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "publication.html#when",
    "href": "publication.html#when",
    "title": "Data Publication",
    "section": "When",
    "text": "When\nIt would be appropriate to carry out data publishing at the end of your project. However, if your repository allows updates to the data package, you can publish at any stage. Publishing earlier can be beneficial, as it provides room to develop and refine the data package over time rather than relying on a last-minute effort.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "publication.html#where",
    "href": "publication.html#where",
    "title": "Data Publication",
    "section": "Where",
    "text": "Where\nYou can use the UU Repository Finder to select an appropriate publishing platform. For the DoY Community, DataverseNL & YODA would be suitable repositories.\nWhen you publish (meta)data on these platforms, you receive a landing page for your dataset and a DOI (persistent identifier), making it easily findable and citable.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "publication.html#how",
    "href": "publication.html#how",
    "title": "Data Publication",
    "section": "How",
    "text": "How\nOnce you’ve selected a repository using the UU Repository Finder, you can work on:\n\nAdding structured metadata and uploading documentation, which further enhance the dataset’s accessibility and reusability.\nThe data files themselves can be withheld internally or placed under restricted access, while the metadata and documentation are openly published. Once any data sharing agreements are signed, the data can be shared using the appropriate tool/infrastructure.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "publication.html#examples",
    "href": "publication.html#examples",
    "title": "Data Publication",
    "section": "Examples",
    "text": "Examples\nThe following data publications are aligned with the ‘as open as possible, as closed as necessary’ (European Commission) principle:\n\nNijhof, Sanne; Putte, Elise van de; Hoefnagels, Johanna Wilhelmina, 2021, “PROactive Cohort Study”, https://doi.org/10.34894/FXUGHW, DataverseNL, V3\nIsabelle van der Linden; Henk Schipper; Sanne Nijhof; Kors van der Ent, 2024, “SMART-Youth: Data”, https://doi.org/10.34894/FCBXSI, DataverseNL, V1",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Publication"
    ]
  },
  {
    "objectID": "transfer.html",
    "href": "transfer.html",
    "title": "Data Transfer",
    "section": "",
    "text": "What\nData Transfer refers to the process of making data available to an approved re-user, either by handing over the data directly or by providing secure access so they can work with it.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Transfer"
    ]
  },
  {
    "objectID": "transfer.html#why",
    "href": "transfer.html#why",
    "title": "Data Transfer",
    "section": "Why",
    "text": "Why\nIf your data are privacy-sensitive, they may be withheld internally or placed under restricted access at the publication stage. Since the data cannot be downloaded via the repository, alternative tools must be used to make the data available to the re-user. This process is typically accompanied by formal agreements and documentation.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Transfer"
    ]
  },
  {
    "objectID": "transfer.html#who",
    "href": "transfer.html#who",
    "title": "Data Transfer",
    "section": "Who",
    "text": "Who\nThe individuals responsible for reviewing data requests + preparing and transferring data, as specified in the Data Governance framework / Data Access Protocol. Privacy Officers may need to be consulted, and data stewards can provide support on appropriate procedures and tools.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Transfer"
    ]
  },
  {
    "objectID": "transfer.html#when",
    "href": "transfer.html#when",
    "title": "Data Transfer",
    "section": "When",
    "text": "When\nThese procedures take place as data requests are received and preliminarily approved.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Transfer"
    ]
  },
  {
    "objectID": "transfer.html#where",
    "href": "transfer.html#where",
    "title": "Data Transfer",
    "section": "Where",
    "text": "Where\nWithin your project folder, it would be good practice to maintain a dedicated subfolder for materials related to data sharing. This can include, but need not be limited to:\n\na data sharing logbook\ncopies of data request forms, correspondence, and agreements\na copy of the shared dataset or the script used to generate the shared subset",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Transfer"
    ]
  },
  {
    "objectID": "transfer.html#how",
    "href": "transfer.html#how",
    "title": "Data Transfer",
    "section": "How",
    "text": "How\nIn order to share data with re-users, you will need to consider the following:\n\nInformation & Consent: The information letters and informed consent forms provided to participants must clearly state whether data sharing and reuse are permitted.\nRisk Assessment: A Data Protection Impact Assessment (DPIA) may be required to determine whether data can be shared safely and under what conditions.\nLegal Agreements: Any transfer of data outside the university requires a Data Transfer Agreement (DTA) in line with the GDPR. The complexity of the DTA depends on the nature of the transfer, for example whether data are shared outside the EU.\nTools: Appropriate tool(s) for data transfer should be selected based on the outcomes of the aforementioned assessments and reviews.\n\n\nTools\n\nSURFfilesender\nSURFfilesender allows you to transfer large files (up to 1 TB) securely. The tool is GDPR-compliant and offers the option of encryption for additional security. You can log in using your institutional credentials. Moreover, you can provide guest access to users without an institutional account or eduID (for example, societal partners / external collaborators) if they need to send you files securely.\n\n\nVirtual Research Environments\nVirtual Research Environments (VREs) — such as those provided by ResearchCloud (UU) and anDREa (UMCU) — are preconfigured workspaces that enable collaboration on and reuse of data. These workspaces can be accessed from anywhere in the world via the provider’s web portal. Depending on your requirements, they can be configured with additional security measures, such as preventing data from being downloaded to a re-user’s computer.",
    "crumbs": [
      "PUBLISH & SHARE",
      "Data Transfer"
    ]
  },
  {
    "objectID": "searching.html",
    "href": "searching.html",
    "title": "Searching for Data",
    "section": "",
    "text": "What\nAs open science becomes the standard in academia, more (meta)data are published in repositories across the web. Before creating new data, it is therefore worth searching for existing datasets that may be reused or built upon.",
    "crumbs": [
      "DISCOVER & REUSE",
      "Searching for Data"
    ]
  },
  {
    "objectID": "searching.html#why",
    "href": "searching.html#why",
    "title": "Searching for Data",
    "section": "Why",
    "text": "Why\nSearching for existing data can save time and resources while enabling comparison and improvements in research methodology and data quality. That being said, existing datasets are often collected for specific study designs and this may require adapting your research questions or methods to ensure compatibility.",
    "crumbs": [
      "DISCOVER & REUSE",
      "Searching for Data"
    ]
  },
  {
    "objectID": "searching.html#who",
    "href": "searching.html#who",
    "title": "Searching for Data",
    "section": "Who",
    "text": "Who\nResearchers at all career stages can benefit from searching for existing data. Librarians and data stewards can provide support in identifying suitable repositories and search strategies.",
    "crumbs": [
      "DISCOVER & REUSE",
      "Searching for Data"
    ]
  },
  {
    "objectID": "searching.html#when",
    "href": "searching.html#when",
    "title": "Searching for Data",
    "section": "When",
    "text": "When\nSearching for data is especially useful in the early stages of a project, such as during proposal writing or study design. However, it can also be valuable later on, for example when validating results, performing comparisons, or extending analyses.",
    "crumbs": [
      "DISCOVER & REUSE",
      "Searching for Data"
    ]
  },
  {
    "objectID": "searching.html#where",
    "href": "searching.html#where",
    "title": "Searching for Data",
    "section": "Where",
    "text": "Where\nData can be deposited in generic or domain-specific repositories. They may also be curated and described within catalogues.\nExamples of generic repositories include DataVerseNL, Yoda, and Zenodo.\nFor the social sciences and humanities, national infrastructures such as ODISSEI and CLARIAH serve as catalogues that provide access to datasets.\nRegistries such as re3data and FAIRsharing can help you identify suitable repositories for your discipline.\nIn addition, the following discovery platforms can help you find datasets across repositories:\n\nOpenAlex: a database containing a wide range of scholarly outputs, including datasets.\n\nDataCite Commons: a discovery service aimed at making research outputs findable and connected.\n\nOpenAIRE and its Dutch counterpart, the Netherlands Research Portal.\n\nOpen Data Europe: a portal providing access to a large collection of European datasets from the EU and beyond.",
    "crumbs": [
      "DISCOVER & REUSE",
      "Searching for Data"
    ]
  },
  {
    "objectID": "searching.html#how",
    "href": "searching.html#how",
    "title": "Searching for Data",
    "section": "How",
    "text": "How\nYou can search for data by constructing BOOLEAN search queries, just as you would in a systematic literature search. Operators such as AND, OR, NOT, and wildcards (e.g. *) can be used to combine concepts, broaden or narrow results, and capture variations of search terms.",
    "crumbs": [
      "DISCOVER & REUSE",
      "Searching for Data"
    ]
  },
  {
    "objectID": "searching.html#references",
    "href": "searching.html#references",
    "title": "Searching for Data",
    "section": "References",
    "text": "References\n\nhttps://www.uu.nl/en/research/research-data-management/tools-services/finding-existing-data\nhttps://dmeg.cessda.eu/Data-Management-Expert-Guide/7.-Discover/The-process-of-data-discovery\nhttps://rdm.vu.nl/topics/finding-existing-data.html",
    "crumbs": [
      "DISCOVER & REUSE",
      "Searching for Data"
    ]
  },
  {
    "objectID": "trainings.html",
    "href": "trainings.html",
    "title": "Trainings",
    "section": "",
    "text": "2024",
    "crumbs": [
      "APPENDIX",
      "Trainings"
    ]
  },
  {
    "objectID": "trainings.html#section",
    "href": "trainings.html#section",
    "title": "Trainings",
    "section": "",
    "text": "Managing Qualitative Data",
    "crumbs": [
      "APPENDIX",
      "Trainings"
    ]
  },
  {
    "objectID": "trainings.html#section-1",
    "href": "trainings.html#section-1",
    "title": "Trainings",
    "section": "2023",
    "text": "2023\n\nCAS Data Collection",
    "crumbs": [
      "APPENDIX",
      "Trainings"
    ]
  },
  {
    "objectID": "trainings.html#section-2",
    "href": "trainings.html#section-2",
    "title": "Trainings",
    "section": "2022",
    "text": "2022\n\nCAS Data Collection",
    "crumbs": [
      "APPENDIX",
      "Trainings"
    ]
  },
  {
    "objectID": "trainings.html#section-3",
    "href": "trainings.html#section-3",
    "title": "Trainings",
    "section": "2021",
    "text": "2021\n\nCAS Data Collection",
    "crumbs": [
      "APPENDIX",
      "Trainings"
    ]
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "2023",
    "crumbs": [
      "APPENDIX",
      "Presentations"
    ]
  },
  {
    "objectID": "presentations.html#section",
    "href": "presentations.html#section",
    "title": "Presentations",
    "section": "",
    "text": "Open Science on Track\n\n\n\nDoY Network Lunch",
    "crumbs": [
      "APPENDIX",
      "Presentations"
    ]
  },
  {
    "objectID": "presentations.html#section-1",
    "href": "presentations.html#section-1",
    "title": "Presentations",
    "section": "2022",
    "text": "2022\n\nOS Platform",
    "crumbs": [
      "APPENDIX",
      "Presentations"
    ]
  },
  {
    "objectID": "presentations.html#section-2",
    "href": "presentations.html#section-2",
    "title": "Presentations",
    "section": "2021",
    "text": "2021\n\nChild Health\n\n\n\nOSCoffee",
    "crumbs": [
      "APPENDIX",
      "Presentations"
    ]
  }
]